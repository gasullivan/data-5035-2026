{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "id": "7e3f7f1b-ae78-41ae-a4cd-e2e8e41a86a0",
      "metadata": {
        "language": "python"
      },
      "source": "# ==============================================================================\n# FILE: donations_eda.py\n# PURPOSE: Exploratory Data Analysis for DONATIONS table\n# AUTHOR: Greg Sullivan\n# DATE: January 2026\n# DESCRIPTION: Comprehensive EDA to identify data quality issues, patterns,\n#              and anomalies in the donations dataset before conducting formal\n#              SQL-based data quality analysis.\n# ==============================================================================\n\n# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Set display options for better readability\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\n\n# Set visualization style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# ==============================================================================\n# SECTION 1: Data Loading and Initial Inspection\n# ==============================================================================\n\ndef load_and_inspect_data(file_path):\n    \"\"\"\n    Load the donations CSV file and perform initial inspection.\n    \n    NOTE: Uses keep_default_na=False to prevent Pandas from automatically\n    converting 'N/A' strings to NaN values, which matches Snowflake's behavior.\n    \n    Parameters:\n    -----------\n    file_path : str\n        Path to the donations.csv file\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Loaded donations data\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"SECTION 1: DATA LOADING AND INITIAL INSPECTION\")\n    print(\"=\" * 80)\n    \n    # Load data - prevent automatic N/A conversion to match Snowflake behavior\n    df = pd.read_csv(file_path, keep_default_na=False, na_values=[''])\n    \n    print(f\"\\n1.1 Dataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n    \n    print(\"\\n1.2 Column Names and Data Types:\")\n    print(df.dtypes)\n    \n    print(\"\\n1.3 First 10 Records:\")\n    print(df.head(10))\n    \n    print(\"\\n1.4 Last 10 Records:\")\n    print(df.tail(10))\n    \n    print(\"\\n1.5 Random Sample of 10 Records:\")\n    print(df.sample(10, random_state=42))\n    \n    print(\"\\n1.6 Basic Info:\")\n    print(df.info())\n    \n    return df\n\n\n# ==============================================================================\n# SECTION 2: Missing Data Analysis\n# ==============================================================================\n\ndef analyze_missing_data(df):\n    \"\"\"\n    Comprehensive analysis of missing and null values.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 2: MISSING DATA ANALYSIS\")\n    print(\"=\" * 80)\n    \n    # Count missing values\n    missing_counts = df.isnull().sum()\n    missing_percentages = (df.isnull().sum() / len(df) * 100).round(2)\n    \n    missing_df = (\n        pd.DataFrame({\n            'column': missing_counts.index,\n            'missing_count': missing_counts.values,\n            'missing_percentage': missing_percentages.values\n        })\n        .sort_values('missing_count', ascending=False)\n        .reset_index(drop=True)\n    )\n    \n    print(\"\\n2.1 Missing Values Summary:\")\n    print(missing_df)\n    \n    # Check for empty strings or whitespace-only values\n    print(\"\\n2.2 Empty String or Whitespace-Only Values:\")\n    for col in df.select_dtypes(include=['object']).columns:\n        empty_count = (df[col].str.strip() == '').sum()\n        whitespace_count = df[col].str.isspace().sum() if df[col].dtype == 'object' else 0\n        \n        if empty_count > 0 or whitespace_count > 0:\n            print(f\"   {col}:\")\n            print(f\"      - Empty strings: {empty_count}\")\n            print(f\"      - Whitespace only: {whitespace_count}\")\n    \n    # Visualize missing data\n    if missing_counts.sum() > 0:\n        plt.figure(figsize=(10, 6))\n        missing_df_plot = missing_df[missing_df['missing_count'] > 0]\n        \n        if not missing_df_plot.empty:\n            plt.barh(missing_df_plot['column'], missing_df_plot['missing_percentage'])\n            plt.xlabel('Missing Percentage (%)')\n            plt.title('Missing Data by Column')\n            plt.tight_layout()\n            plt.show()\n\n\n# ==============================================================================\n# SECTION 3: Descriptive Statistics\n# ==============================================================================\n\ndef generate_descriptive_statistics(df):\n    \"\"\"\n    Generate comprehensive descriptive statistics for all columns.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 3: DESCRIPTIVE STATISTICS\")\n    print(\"=\" * 80)\n    \n    # Numeric columns\n    print(\"\\n3.1 Numeric Column Statistics:\")\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    print(df[numeric_cols].describe())\n    \n    # Categorical columns\n    print(\"\\n3.2 Categorical Column Statistics:\")\n    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n    \n    for col in categorical_cols:\n        print(f\"\\n   {col}:\")\n        print(f\"      - Unique values: {df[col].nunique()}\")\n        print(f\"      - Most common: {df[col].mode()[0] if not df[col].mode().empty else 'N/A'}\")\n        print(f\"      - Value counts (top 10):\")\n        print(df[col].value_counts().head(10))\n\n\n# ==============================================================================\n# SECTION 4: Data Quality Issues - Name Format Analysis\n# ==============================================================================\n\ndef analyze_name_formats(df):\n    \"\"\"\n    Analyze NAME column for format inconsistencies.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 4: NAME FORMAT ANALYSIS\")\n    print(\"=\" * 80)\n    \n    # Check for comma presence (Last, First format)\n    df_analysis = df.copy()\n    df_analysis['has_comma'] = df_analysis['NAME'].str.contains(',', na=False)\n    \n    comma_count = df_analysis['has_comma'].sum()\n    no_comma_count = len(df_analysis) - comma_count\n    \n    print(f\"\\n4.1 Name Format Distribution:\")\n    print(f\"   - Names with comma (Last, First): {comma_count} ({comma_count/len(df)*100:.1f}%)\")\n    print(f\"   - Names without comma (First Last): {no_comma_count} ({no_comma_count/len(df)*100:.1f}%)\")\n    \n    print(f\"\\n4.2 Sample Names WITH Comma:\")\n    print(df_analysis[df_analysis['has_comma']]['NAME'].head(10).tolist())\n    \n    print(f\"\\n4.3 Sample Names WITHOUT Comma:\")\n    print(df_analysis[~df_analysis['has_comma']]['NAME'].head(10).tolist())\n    \n    # Name length analysis\n    df_analysis['name_length'] = df_analysis['NAME'].str.len()\n    \n    print(f\"\\n4.4 Name Length Statistics:\")\n    print(df_analysis['name_length'].describe())\n    \n    # Visualize\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Pie chart for format distribution\n    axes[0].pie(\n        [comma_count, no_comma_count],\n        labels=['Last, First', 'First Last'],\n        autopct='%1.1f%%',\n        startangle=90\n    )\n    axes[0].set_title('Name Format Distribution')\n    \n    # Histogram of name lengths\n    axes[1].hist(df_analysis['name_length'], bins=20, edgecolor='black')\n    axes[1].set_xlabel('Name Length')\n    axes[1].set_ylabel('Frequency')\n    axes[1].set_title('Distribution of Name Lengths')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# ==============================================================================\n# SECTION 5: Data Quality Issues - Category Analysis\n# ==============================================================================\n\ndef analyze_categories(df):\n    \"\"\"\n    Analyze CATEGORY column for data quality issues.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 5: CATEGORY ANALYSIS\")\n    print(\"=\" * 80)\n    \n    df_analysis = df.copy()\n    \n    # Clean and analyze\n    print(\"\\n5.1 Category Value Counts:\")\n    print(df_analysis['CATEGORY'].value_counts(dropna=False))\n    \n    # Check for blank/empty categories (matching Snowflake's TRIM behavior)\n    # Snowflake counts: NULL OR TRIM(category) = ''\n    blank_or_null = df_analysis['CATEGORY'].isnull() | (df_analysis['CATEGORY'].str.strip() == '')\n    blank_count = blank_or_null.sum()\n    \n    print(f\"\\n5.2 Missing/Blank Category Analysis (matches Snowflake):\")\n    print(f\"   - NULL or empty after TRIM: {blank_count} ({blank_count/len(df)*100:.1f}%)\")\n    \n    # Placeholder values - now correctly identifying 'N/A' as a string\n    na_count = (df_analysis['CATEGORY'] == 'N/A').sum()\n    unknown_count = (df_analysis['CATEGORY'] == 'Unknown').sum()\n    placeholder_count = na_count + unknown_count\n    \n    print(f\"\\n5.3 Placeholder Values:\")\n    print(f\"   - 'N/A': {na_count}\")\n    print(f\"   - 'Unknown': {unknown_count}\")\n    print(f\"   - Total placeholders: {placeholder_count} ({placeholder_count/len(df)*100:.1f}%)\")\n    \n    # Total problematic\n    total_problematic = blank_count + placeholder_count\n    print(f\"\\n5.4 Total Problematic Categories:\")\n    print(f\"   - Missing/blank + placeholders: {total_problematic} ({total_problematic/len(df)*100:.1f}%)\")\n    \n    # Check for multi-word categories (inconsistent naming)\n    df_analysis['has_space'] = df_analysis['CATEGORY'].str.contains(' ', na=False)\n    multi_word_count = df_analysis['has_space'].sum()\n    \n    print(f\"\\n5.5 Category Naming Convention:\")\n    print(f\"   - Multi-word categories: {multi_word_count}\")\n    print(f\"   - Single-word categories: {len(df_analysis) - multi_word_count - blank_count}\")\n    \n    # Visualize\n    category_counts = df_analysis['CATEGORY'].value_counts().head(10)\n    \n    plt.figure(figsize=(12, 6))\n    plt.barh(category_counts.index, category_counts.values)\n    plt.xlabel('Count')\n    plt.title('Top 10 Categories by Frequency')\n    plt.tight_layout()\n    plt.show()\n\n\n# ==============================================================================\n# SECTION 6: Data Quality Issues - Age and Date of Birth Analysis\n# ==============================================================================\n\ndef analyze_age_dob(df):\n    \"\"\"\n    Analyze AGE and DATE_OF_BIRTH columns for mismatches.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 6: AGE AND DATE OF BIRTH ANALYSIS\")\n    print(\"=\" * 80)\n    \n    df_analysis = df.copy()\n    \n    # Parse dates\n    df_analysis['dob_parsed'] = pd.to_datetime(\n        df_analysis['DATE_OF_BIRTH'], \n        format='%m/%d/%y',\n        errors='coerce'\n    )\n    \n    # Handle 2-digit year ambiguity (assume >24 means 1900s, <=24 means 2000s)\n    current_year = 2024\n    df_analysis['birth_year'] = df_analysis['dob_parsed'].dt.year\n    df_analysis['birth_year_adjusted'] = df_analysis['birth_year'].apply(\n        lambda x: x - 100 if x > current_year else x\n    )\n    \n    # Calculate age from DOB\n    df_analysis['calculated_age'] = current_year - df_analysis['birth_year_adjusted']\n    \n    # Calculate difference\n    df_analysis['age_difference'] = abs(df_analysis['AGE'] - df_analysis['calculated_age'])\n    \n    print(\"\\n6.1 Age Statistics:\")\n    print(df_analysis['AGE'].describe())\n    \n    print(\"\\n6.2 Calculated Age Statistics:\")\n    print(df_analysis['calculated_age'].describe())\n    \n    print(\"\\n6.3 Age Difference Statistics:\")\n    print(df_analysis['age_difference'].describe())\n    \n    # Mismatches (>5 years difference)\n    mismatched = df_analysis[df_analysis['age_difference'] > 5]\n    print(f\"\\n6.4 Age/DOB Mismatches (>5 years difference):\")\n    print(f\"   - Count: {len(mismatched)} ({len(mismatched)/len(df)*100:.1f}%)\")\n    \n    print(f\"\\n6.5 Sample Mismatched Records:\")\n    print(mismatched[['DONATION_ID', 'NAME', 'AGE', 'DATE_OF_BIRTH', 'birth_year_adjusted', \n                      'calculated_age', 'age_difference']].head(10))\n    \n    # Check for round ages\n    round_ages = df_analysis[df_analysis['AGE'] % 5 == 0]\n    print(f\"\\n6.6 Round Ages (multiples of 5):\")\n    print(f\"   - Count: {len(round_ages)} ({len(round_ages)/len(df)*100:.1f}%)\")\n    print(f\"   - This suggests possible age estimation rather than calculation\")\n    \n    # Visualizations\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Age distribution\n    axes[0, 0].hist(df_analysis['AGE'], bins=20, edgecolor='black')\n    axes[0, 0].set_xlabel('Age')\n    axes[0, 0].set_ylabel('Frequency')\n    axes[0, 0].set_title('Distribution of Reported Ages')\n    \n    # Calculated age distribution\n    axes[0, 1].hist(df_analysis['calculated_age'].dropna(), bins=20, edgecolor='black')\n    axes[0, 1].set_xlabel('Calculated Age')\n    axes[0, 1].set_ylabel('Frequency')\n    axes[0, 1].set_title('Distribution of Calculated Ages (from DOB)')\n    \n    # Age difference\n    axes[1, 0].hist(df_analysis['age_difference'].dropna(), bins=30, edgecolor='black')\n    axes[1, 0].axvline(x=5, color='red', linestyle='--', label='5-year threshold')\n    axes[1, 0].set_xlabel('Age Difference (years)')\n    axes[1, 0].set_ylabel('Frequency')\n    axes[1, 0].set_title('Distribution of Age/DOB Differences')\n    axes[1, 0].legend()\n    \n    # Scatter: Reported vs Calculated\n    axes[1, 1].scatter(df_analysis['calculated_age'], df_analysis['AGE'], alpha=0.5)\n    axes[1, 1].plot([0, 100], [0, 100], 'r--', label='Perfect match')\n    axes[1, 1].set_xlabel('Calculated Age')\n    axes[1, 1].set_ylabel('Reported Age')\n    axes[1, 1].set_title('Reported Age vs Calculated Age')\n    axes[1, 1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# ==============================================================================\n# SECTION 7: Data Quality Issues - Phone Number Analysis\n# ==============================================================================\n\ndef analyze_phone_numbers(df):\n    \"\"\"\n    Analyze PHONE column for format inconsistencies.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 7: PHONE NUMBER ANALYSIS\")\n    print(\"=\" * 80)\n    \n    df_analysis = df.copy()\n    \n    # Extract digits only\n    df_analysis['phone_digits'] = df_analysis['PHONE'].str.replace(r'\\D', '', regex=True)\n    df_analysis['phone_digit_count'] = df_analysis['phone_digits'].str.len()\n    df_analysis['phone_length'] = df_analysis['PHONE'].str.len()\n    \n    print(\"\\n7.1 Phone Number Length Statistics:\")\n    print(df_analysis['phone_length'].describe())\n    \n    print(\"\\n7.2 Digit Count Statistics:\")\n    print(df_analysis['phone_digit_count'].describe())\n    \n    print(\"\\n7.3 Digit Count Distribution:\")\n    print(df_analysis['phone_digit_count'].value_counts().sort_index())\n    \n    # Identify format patterns\n    df_analysis['has_parentheses'] = df_analysis['PHONE'].str.contains(r'\\(', na=False)\n    df_analysis['has_dash'] = df_analysis['PHONE'].str.contains('-', na=False)\n    df_analysis['has_dot'] = df_analysis['PHONE'].str.contains(r'\\.', na=False)\n    df_analysis['has_extension'] = df_analysis['PHONE'].str.contains('x', na=False)\n    df_analysis['has_plus'] = df_analysis['PHONE'].str.contains(r'\\+', na=False)\n    \n    print(\"\\n7.4 Phone Format Patterns:\")\n    print(f\"   - With parentheses: {df_analysis['has_parentheses'].sum()}\")\n    print(f\"   - With dashes: {df_analysis['has_dash'].sum()}\")\n    print(f\"   - With dots: {df_analysis['has_dot'].sum()}\")\n    print(f\"   - With extensions: {df_analysis['has_extension'].sum()}\")\n    print(f\"   - With plus sign (intl): {df_analysis['has_plus'].sum()}\")\n    \n    # Invalid phone numbers (not 10 digits)\n    invalid_phones = df_analysis[df_analysis['phone_digit_count'] != 10]\n    print(f\"\\n7.5 Invalid Phone Numbers (not 10 digits):\")\n    print(f\"   - Count: {len(invalid_phones)} ({len(invalid_phones)/len(df)*100:.1f}%)\")\n    print(f\"\\n   Sample Invalid Phone Numbers:\")\n    print(invalid_phones[['DONATION_ID', 'NAME', 'PHONE', 'phone_digit_count']].head(10))\n    \n    # Visualize\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Digit count distribution\n    axes[0].hist(df_analysis['phone_digit_count'], bins=range(0, 15), edgecolor='black')\n    axes[0].axvline(x=10, color='red', linestyle='--', label='Expected (10 digits)')\n    axes[0].set_xlabel('Number of Digits')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Distribution of Phone Number Digit Counts')\n    axes[0].legend()\n    \n    # Format patterns\n    format_counts = pd.Series({\n        'Parentheses': df_analysis['has_parentheses'].sum(),\n        'Dashes': df_analysis['has_dash'].sum(),\n        'Dots': df_analysis['has_dot'].sum(),\n        'Extensions': df_analysis['has_extension'].sum(),\n        'Plus/Intl': df_analysis['has_plus'].sum()\n    })\n    \n    axes[1].barh(format_counts.index, format_counts.values)\n    axes[1].set_xlabel('Count')\n    axes[1].set_title('Phone Number Format Patterns')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# ==============================================================================\n# SECTION 8: Data Quality Issues - Donation Amount Analysis\n# ==============================================================================\n\ndef analyze_donation_amounts(df):\n    \"\"\"\n    Analyze AMOUNT column for outliers and patterns.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 8: DONATION AMOUNT ANALYSIS\")\n    print(\"=\" * 80)\n    \n    df_analysis = df.copy()\n    \n    print(\"\\n8.1 Amount Statistics:\")\n    print(df_analysis['AMOUNT'].describe())\n    \n    print(\"\\n8.2 Percentile Analysis:\")\n    percentiles = [0.5, 0.75, 0.90, 0.95, 0.99, 1.0]\n    for p in percentiles:\n        value = df_analysis['AMOUNT'].quantile(p)\n        print(f\"   - {p*100:.0f}th percentile: ${value:,.2f}\")\n    \n    # Outliers (>$1M)\n    outliers = df_analysis[df_analysis['AMOUNT'] > 1000000]\n    print(f\"\\n8.3 Extreme Outliers (>$1,000,000):\")\n    print(f\"   - Count: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n    print(f\"   - Total value: ${outliers['AMOUNT'].sum():,.2f}\")\n    print(f\"\\n   Outlier Records:\")\n    print(outliers[['DONATION_ID', 'NAME', 'ORGANIZATION', 'AMOUNT']].sort_values('AMOUNT', ascending=False))\n    \n    # Impact on statistics\n    total_with_outliers = df_analysis['AMOUNT'].sum()\n    total_without_outliers = df_analysis[df_analysis['AMOUNT'] <= 1000000]['AMOUNT'].sum()\n    \n    print(f\"\\n8.4 Impact of Outliers:\")\n    print(f\"   - Total donations (with outliers): ${total_with_outliers:,.2f}\")\n    print(f\"   - Total donations (without outliers): ${total_without_outliers:,.2f}\")\n    print(f\"   - Outliers represent {(total_with_outliers - total_without_outliers)/total_with_outliers*100:.1f}% of total value\")\n    \n    # Visualizations\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Full distribution (with outliers)\n    axes[0, 0].hist(df_analysis['AMOUNT'], bins=50, edgecolor='black')\n    axes[0, 0].set_xlabel('Amount ($)')\n    axes[0, 0].set_ylabel('Frequency')\n    axes[0, 0].set_title('Donation Amount Distribution (Full Range)')\n    \n    # Distribution without outliers\n    normal_range = df_analysis[df_analysis['AMOUNT'] <= 1000]\n    axes[0, 1].hist(normal_range['AMOUNT'], bins=50, edgecolor='black')\n    axes[0, 1].set_xlabel('Amount ($)')\n    axes[0, 1].set_ylabel('Frequency')\n    axes[0, 1].set_title('Donation Amount Distribution (≤$1,000)')\n    \n    # Box plot\n    axes[1, 0].boxplot(df_analysis['AMOUNT'], vert=False)\n    axes[1, 0].set_xlabel('Amount ($)')\n    axes[1, 0].set_title('Box Plot of Donation Amounts')\n    \n    # Log scale histogram\n    axes[1, 1].hist(np.log10(df_analysis['AMOUNT'] + 1), bins=50, edgecolor='black')\n    axes[1, 1].set_xlabel('log10(Amount + 1)')\n    axes[1, 1].set_ylabel('Frequency')\n    axes[1, 1].set_title('Log-Scale Distribution of Donation Amounts')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# ==============================================================================\n# SECTION 9: Data Quality Issues - ZIP Code Analysis\n# ==============================================================================\n\ndef analyze_zip_codes(df):\n    \"\"\"\n    Analyze ZIP column for incomplete codes.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 9: ZIP CODE ANALYSIS\")\n    print(\"=\" * 80)\n    \n    df_analysis = df.copy()\n    \n    # Convert to string and get length\n    df_analysis['zip_str'] = df_analysis['ZIP'].astype(str)\n    df_analysis['zip_length'] = df_analysis['zip_str'].str.len()\n    \n    print(\"\\n9.1 ZIP Code Length Distribution:\")\n    print(df_analysis['zip_length'].value_counts().sort_index())\n    \n    # Incomplete ZIPs\n    incomplete_zips = df_analysis[df_analysis['zip_length'] < 5]\n    print(f\"\\n9.2 Incomplete ZIP Codes (<5 digits):\")\n    print(f\"   - Count: {len(incomplete_zips)} ({len(incomplete_zips)/len(df)*100:.1f}%)\")\n    print(f\"\\n   Sample Incomplete ZIPs:\")\n    print(incomplete_zips[['DONATION_ID', 'NAME', 'CITY', 'STATE', 'ZIP', 'zip_length']].head(10))\n    \n    # State distribution\n    print(\"\\n9.3 Records by State (Top 10):\")\n    print(df_analysis['STATE'].value_counts().head(10))\n    \n    # Visualize\n    plt.figure(figsize=(10, 6))\n    df_analysis['zip_length'].value_counts().sort_index().plot(kind='bar', edgecolor='black')\n    plt.xlabel('ZIP Code Length')\n    plt.ylabel('Count')\n    plt.title('Distribution of ZIP Code Lengths')\n    plt.axvline(x=4.5, color='red', linestyle='--', label='Expected (5 digits)')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n# ==============================================================================\n# SECTION 10: Data Quality Issues - Street Address Analysis\n# ==============================================================================\n\ndef analyze_street_addresses(df):\n    \"\"\"\n    Analyze STREET_ADDRESS for excessive detail or anomalies.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 10: STREET ADDRESS ANALYSIS\")\n    print(\"=\" * 80)\n    \n    df_analysis = df.copy()\n    \n    df_analysis['address_length'] = df_analysis['STREET_ADDRESS'].str.len()\n    \n    print(\"\\n10.1 Address Length Statistics:\")\n    print(df_analysis['address_length'].describe())\n    \n    # Long addresses\n    long_addresses = df_analysis[df_analysis['address_length'] > 50]\n    print(f\"\\n10.2 Excessively Long Addresses (>50 characters):\")\n    print(f\"   - Count: {len(long_addresses)} ({len(long_addresses)/len(df)*100:.1f}%)\")\n    print(f\"\\n   Sample Long Addresses:\")\n    print(long_addresses[['DONATION_ID', 'NAME', 'STREET_ADDRESS', 'address_length']].head(10))\n    \n    # Check for common patterns\n    df_analysis['has_apt'] = df_analysis['STREET_ADDRESS'].str.contains('Apt', na=False)\n    df_analysis['has_suite'] = df_analysis['STREET_ADDRESS'].str.contains('Suite', na=False)\n    \n    print(f\"\\n10.3 Address Patterns:\")\n    print(f\"   - With 'Apt': {df_analysis['has_apt'].sum()}\")\n    print(f\"   - With 'Suite': {df_analysis['has_suite'].sum()}\")\n    \n    # Visualize\n    plt.figure(figsize=(10, 6))\n    plt.hist(df_analysis['address_length'], bins=30, edgecolor='black')\n    plt.axvline(x=50, color='red', linestyle='--', label='Threshold (50 chars)')\n    plt.xlabel('Address Length')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Street Address Lengths')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n# ==============================================================================\n# SECTION 11: Overall Data Quality Summary\n# ==============================================================================\n\ndef generate_overall_summary(df):\n    \"\"\"\n    Generate comprehensive data quality summary report.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The donations dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SECTION 11: OVERALL DATA QUALITY SUMMARY\")\n    print(\"=\" * 80)\n    \n    total_records = len(df)\n    \n    # Calculate all DQ issues\n    dq_issues = {\n        'Reversed Names (Last, First)': df['NAME'].str.contains(',', na=False).sum(),\n        'Missing/Blank Categories': (df['CATEGORY'].isnull() | (df['CATEGORY'].str.strip() == '')).sum(),\n        'Placeholder N/A': (df['CATEGORY'] == 'N/A').sum(),\n        'Placeholder Unknown': (df['CATEGORY'] == 'Unknown').sum(),\n        'Age/DOB Mismatch (>5 years)': None,  # Calculated separately\n        'Invalid Phone Format': None,  # Calculated separately\n        'Outlier Amounts (>$1M)': (df['AMOUNT'] > 1000000).sum(),\n        'Incomplete ZIP Codes': (df['ZIP'].astype(str).str.len() < 5).sum(),\n        'Long Addresses (>50 chars)': (df['STREET_ADDRESS'].str.len() > 50).sum(),\n    }\n    \n    # Age/DOB mismatch calculation\n    df_temp = df.copy()\n    df_temp['dob_parsed'] = pd.to_datetime(df_temp['DATE_OF_BIRTH'], format='%m/%d/%y', errors='coerce')\n    df_temp['birth_year'] = df_temp['dob_parsed'].dt.year\n    df_temp['birth_year_adjusted'] = df_temp['birth_year'].apply(lambda x: x - 100 if x > 2024 else x)\n    df_temp['calculated_age'] = 2024 - df_temp['birth_year_adjusted']\n    df_temp['age_difference'] = abs(df_temp['AGE'] - df_temp['calculated_age'])\n    dq_issues['Age/DOB Mismatch (>5 years)'] = (df_temp['age_difference'] > 5).sum()\n    \n    # Phone format calculation\n    df_temp['phone_digits'] = df_temp['PHONE'].str.replace(r'\\D', '', regex=True).str.len()\n    dq_issues['Invalid Phone Format'] = (df_temp['phone_digits'] != 10).sum()\n    \n    print(\"\\n11.1 Data Quality Issues Summary:\")\n    print(f\"\\n{'Issue':<40} {'Count':<10} {'Percentage'}\")\n    print(\"-\" * 60)\n    \n    for issue, count in sorted(dq_issues.items(), key=lambda x: x[1], reverse=True):\n        pct = (count / total_records * 100)\n        print(f\"{issue:<40} {count:<10} {pct:>6.1f}%\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(f\"TOTAL RECORDS: {total_records}\")\n    print(\"=\" * 80)\n    \n    # Create summary visualization\n    plt.figure(figsize=(12, 8))\n    issues_df = pd.DataFrame(list(dq_issues.items()), columns=['Issue', 'Count'])\n    issues_df = issues_df.sort_values('Count', ascending=True)\n    \n    plt.barh(issues_df['Issue'], issues_df['Count'])\n    plt.xlabel('Number of Records Affected')\n    plt.title('Data Quality Issues - Records Affected by Issue Type')\n    plt.tight_layout()\n    plt.show()\n\n\n# ==============================================================================\n# MAIN EXECUTION\n# ==============================================================================\n\ndef main():\n    \"\"\"\n    Main execution function that runs all EDA sections.\n    \"\"\"\n    # File path\n    file_path = 'donations.csv'\n    \n    # Load data\n    df = load_and_inspect_data(file_path)\n    \n    # Run all analyses\n    analyze_missing_data(df)\n    generate_descriptive_statistics(df)\n    analyze_name_formats(df)\n    analyze_categories(df)\n    analyze_age_dob(df)\n    analyze_phone_numbers(df)\n    analyze_donation_amounts(df)\n    analyze_zip_codes(df)\n    analyze_street_addresses(df)\n    generate_overall_summary(df)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"EDA COMPLETE\")\n    print(\"=\" * 80)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "execution_count": null,
      "outputs": []
    }
  ]
}