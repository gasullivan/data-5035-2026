{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "# Data Engineering - Assignment 04\n## US Top Music Schools Impacted by Severe Winter Weather (January 2026)\n\n**Author:** Greg Sullivan\n\n**Date:** February 2026\n\n**Course:** DATA 5035 - Data Engineering\n\n**Professor:** Paul Boal\n\n---\n\n### Objective\nBuild a data pipeline that combines music conservatory data (from HTML scraping and APIs) with weather data (from API) to estimate student-days impacted by severe winter weather in January 2026.\n\n### My Selected Grouping\n**Top 10 Music Schools in the United States**\n\nSource: [www.thebestschools.org](http://www.thebestschools.org/) - \"The Best Music Schools\" ranking\n\n### Severe Weather Definition\n**A day is considered \"severe winter weather\" if it meets ANY of:**\n- Minimum temperature below 20°F (-6.7°C)\n- Maximum temperature below 32°F (0°C) for entire day\n- Precipitation > 0.5 inches while temperature below 32°F (ice/snow)\n(That's my Midwestern view.  When I worked in Florida I would've had a completely different view!)\n\n### Data Collection Strategy\n**Where options existed, I attempted to collect data in this order:**\n1. **Web Scraping** - Extract directly from school websites\n2. **API** - College Scorecard API for standalone music schools\n3. **Fallback** - Verified data from Common Data Sets (manually entered as a last resort)\n\n### Data Engineering Challenges\n\n**I encountered real-world data collection challenges:**\n- **Network Restrictions:** University policy on cloud platforms had restrictions\n- **API Rate Limits:** College Scorecard API had request limitations (frustrating!)\n- **Inconsistent HTML:** Each school website has unique structure - nothing was consistent\n- **Data in PDFs:** Some schools publish enrollment only in PDF documents or other image formats\n- **403 Errors:** Some sites block automated requests, even when permission was granted\n\n**Why this matters:** A simple Google search shows enrollment data instantly, but programmatic access required navigating rate limits, firewalls, and inconsistent data formats. \n\nI now understand why I've heard data engineers say \"80% of the work is getting the data.\"  This was certainly the case for me on this assignment!",
      "id": "88b811ad-b964-4380-bbe7-acd3f5930ce7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n## Step 1: Import Libraries",
      "id": "81afb21c-a042-4fe7-9ad4-a8e2ba7c96a7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "ImportLibraries",
        "title": "ImportLibraries"
      },
      "outputs": [],
      "source": "import requests                             # for my HTTP requests to APIs\nimport pandas as pd\nfrom bs4 import BeautifulSoup               # for web scraping (will try first)\nimport json\nfrom datetime import datetime, timedelta\nimport time\nimport re                                   # regex for pattern matching in what we grab\n\n# At the end I added a few visualizations\nimport matplotlib.pyplot as plt             # for visualizations\n\nprint(\"Libraries imported successfully\")",
      "id": "c78faeb6-7fdf-46b5-9176-f60192b246df"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n## Step 2: Load School URLs from File",
      "id": "2a87368f-076e-468e-81e7-945cb333e9ce"
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "name": "Top10MusicSchoolsOld",
        "title": "Top10MusicSchoolsOld",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# I created a reference file for the top 10 music schools.  For each school (row in the file) I gathered from general web searches the information I might need later for our analysis.\n# These are all needed in case other attempts to find the data fail.  Regardless of the state of technical matters at the time of execution, I want to assure the analysis still completes.\n# For each school, I captured the following information:\n#   - url\n#   - school_name\n#   - city\n#   - state\n#   - latitude\n#   - longitude\n#   - enrollment (for just the music school, if part of a larger university)\n#   - enrollment_source\n#   - enrollment_year\n\nThese will only be used if our attempts to scrape websites or query APIs fail...",
      "id": "e7d30645-d35e-4cfc-b7c7-9b274f6b7a1a"
    },
    {
      "id": "7db17ca8-8a1d-4908-8ba7-1344057134f5",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Top10MusicSchools",
        "title": "Top10MusicSchools"
      },
      "source": "# Load reference data (contains URLs and all fallback data)\nreference_file = 'Music Schools Reference Data.csv'\nreference_df = pd.read_csv(reference_file)\n\nprint(f\"Loaded {len(reference_df)} schools from reference file:\\n\")\nfor i, row in reference_df.iterrows():\n    print(f\"  {i+1}. {row['url']} {row['school_name']} ({row['city']}, {row['state']})\")\n\n# Extract URLs for scraping attempts\nschool_urls = reference_df['url'].tolist()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false
      },
      "source": "---\n## Step 3: Scrape School Names",
      "id": "a09fc220-fe2c-4d25-823a-a62853d4e9a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "ScrapeSchoolNames",
        "title": "ScrapeSchoolNames"
      },
      "outputs": [],
      "source": "# Function for scraping school name from a given URL\n#   Use reference data if unable to scrape school name\n#   Show results after attempts\ndef scrape_school_name(url):\n    \"\"\"\n    Scrape school name from website using multiple strategies\n    \n    Parameters:\n    -----------\n    url : str\n        School website URL\n    \n    Returns:\n    --------\n    tuple (str or None, str)\n        School name and source/error message\n    \"\"\"\n    try:\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n        response = requests.get(url, timeout=10, headers=headers)\n        if response.status_code != 200:\n            return None, f'HTTP {response.status_code}'\n        \n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Strategy 1: Meta tags\n        for attr, value in [('property', 'og:site_name'), ('property', 'og:title')]:\n            meta = soup.find('meta', {attr: value})\n            if meta and meta.get('content'):\n                name = meta.get('content').strip()\n                name = re.sub(r'\\s*[|•\\-–]\\s*(Home|About|Welcome).*$', '', name, flags=re.IGNORECASE)\n                if name and len(name) > 3 and name.lower() not in ['home', 'about']:\n                    return name, 'meta tag'\n        \n        # Strategy 2: Title tag\n        title = soup.find('title')\n        if title:\n            name = title.get_text(strip=True)\n            name = re.sub(r'\\s*[|•\\-–]\\s*(Home|About|Welcome|Official).*$', '', name, flags=re.IGNORECASE)\n            if name and len(name) > 3 and name.lower() not in ['home', 'about']:\n                return name, 'title tag'\n        \n        return None, 'no name found'\n        \n    except Exception as e:\n        return None, f'Error: {e}'\n\n\n# Fallback school names from reference file (no hardcoding!)\nfallback_names = dict(zip(reference_df['url'], reference_df['school_name']))\n\n\n# Scrape school names\nprint(\"Scraping school names...\\n\")\nschools_data = []\n\nfor url in school_urls:\n    print(f\"Processing {url}...\")\n    \n    name, source = scrape_school_name(url)\n\n    # Scrape first, otherwise use fallback from reference file\n    if name:\n        print(f\"  ✓ SCRAPED: {name} ({source})\")\n        name_source = f'scraped ({source})'\n    else:\n        name = fallback_names.get(url, 'Unknown')\n        print(f\"  → FALLBACK: {name} ({source})\")\n        name_source = f'fallback ({source})'\n    \n    schools_data.append({\n        'url': url,\n        'name': name,\n        'name_source': name_source\n    })\n    \n    time.sleep(1)\n\nschools_df = pd.DataFrame(schools_data)\n\nscraped_count = schools_df['name_source'].str.contains('scraped', case=False).sum()\nprint(f\"\\nSchool Names: {scraped_count}/10 scraped, {10-scraped_count}/10 fallback\")",
      "id": "8061f47a-2a23-49d7-a33d-13fee8ea9f01"
    },
    {
      "id": "80e46f45-7f7d-4799-af9c-5358cb9955d4",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "---\n## Steps 4: Collect Location and Enrollment Data"
    },
    {
      "id": "af6bf3c3-5142-42f8-95d2-75ada59c3147",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# =============================================================================\n# Step 4: Collect Location and Enrollment Data\n# =============================================================================\n# Priority for Enrollment: 1) Web Scraping → 2) College Scorecard API → 3) Reference File\n# Priority for Location: 1) College Scorecard API → 2) Reference File\n#\n# Note: Scraping uses generic patterns that work across most school websites.\n# No site-specific configuration - if generic scraping fails, we fall back to API or reference.\n\n#  This proved incredibly challenging.  I eventually settled on this approach:\n#    1) Try web scraping with Beautiful Soup (lots of experimentation and tuning of the code);\n#    2) If web scraping failed, attempt to gather via the College Scorecard API; and\n#    3) If both of those failed, do a manual Google search and hardcode the enrollment data.\n\n#  One challenge I ran into was in the case a music school was only a part of a larger university.\n#  My intent is to study for this assignment, MUSIC STUDENTS impacted by winter weather, since\n#    they have speciall challenges carrying those instruments around, keeping their hands warm, etc.\n\n\n#  I used some trial-and-error on where best to find the enrollment data as\n#    it was NOT in one consistent location on each university's website\n\n\ndef scrape_enrollment_from_website(url):\n    \"\"\"\n    Scrape enrollment using generic patterns (no site-specific config)\n    Tries common pages and common enrollment text patterns\n    \"\"\"\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n    \n    # Generic pages where enrollment data commonly appears\n    common_paths = ['', '/about', '/facts', '/admissions', '/about/facts-and-statistics']\n    \n    # Generic patterns that match enrollment text across many sites\n    generic_patterns = [\n        r'student\\s+body\\s+of\\s+(\\d+)',\n        r'enrollment[:\\s]+([0-9,]+)',\n        r'([0-9,]+)\\s+students\\s+(?:are\\s+)?enrolled',\n        r'(\\d+)\\s+(?:total\\s+)?students',\n        r'approximately\\s+([0-9,]+)\\s+students',\n        r'around\\s+(\\d+)\\s+students',\n        r'community\\s+of\\s+([0-9,]+)\\s+students',\n        r'([0-9,]+)\\s+collegiate\\s+students',\n    ]\n    \n    # Extract base URL\n    from urllib.parse import urlparse\n    parsed = urlparse(url)\n    base_url = f\"{parsed.scheme}://{parsed.netloc}\"\n    \n    for path in common_paths:\n        try:\n            page_url = base_url + path\n            response = requests.get(page_url, timeout=10, headers=headers)\n            if response.status_code != 200:\n                continue\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            text = soup.get_text()\n            \n            for pattern in generic_patterns:\n                matches = re.findall(pattern, text, re.IGNORECASE)\n                for match in matches:\n                    try:\n                        num = int(match.replace(',', '').replace('+', ''))\n                        # Reasonable enrollment range for music schools\n                        if 50 < num < 10000:\n                            return num, f'scraped ({page_url})'\n                    except:\n                        continue\n        except:\n            continue\n    \n    return None, 'Scraping failed (no pattern matched)'\n\n\ndef fetch_from_college_scorecard(school_name):\n    \"\"\"\n    Fetch location AND enrollment from College Scorecard API\n    \"\"\"\n    # Build search term by removing common music school suffixes\n    search_term = school_name\n    for suffix in [' School of Music', ' College of Music', ' Conservatory of Music', ' Institute of Music']:\n        search_term = search_term.replace(suffix, '')\n    search_term = search_term.strip()\n    \n    api_url = 'https://api.data.gov/ed/collegescorecard/v1/schools.json'\n    params = {\n        'school.name': search_term,\n        'fields': 'school.name,school.city,school.state,location.lat,location.lon,latest.student.size',\n        'api_key': 'DEMO_KEY'\n    }\n    \n    try:\n        response = requests.get(api_url, params=params, timeout=10)\n        if response.status_code == 429:\n            return None, 'API rate limited'\n        if response.status_code != 200:\n            return None, f'HTTP {response.status_code}'\n        \n        data = response.json()\n        results = data.get('results', [])\n        \n        if not results:\n            return None, 'No API results'\n        \n        r = results[0]\n        return {\n            'api_name': r.get('school.name', ''),\n            'city': r.get('school.city'),\n            'state': r.get('school.state'),\n            'latitude': r.get('location.lat'),\n            'longitude': r.get('location.lon'),\n            'enrollment': r.get('latest.student.size')\n        }, 'API'\n        \n    except Exception as e:\n        return None, f'API error: {e}'\n\n\n# Build lookup from reference file (by URL)\nref_by_url = reference_df.set_index('url').to_dict('index')\n\nprint(\"=\"*70)\nprint(\"LOCATION AND ENROLLMENT DATA COLLECTION\")\nprint(\"=\"*70)\nprint(\"\\nEnrollment Priority: 1) Scrape → 2) API → 3) Reference File\")\nprint(\"Location Priority: 1) API → 2) Reference File\\n\")\n\nresults = []\n\nfor idx, row in schools_df.iterrows():\n    school_name = row['name']\n    url = row['url']\n    ref_data = ref_by_url.get(url, {})\n    \n    # Check if API returns standalone school data (from reference file)\n    is_standalone = ref_data.get('is_standalone', True)\n    \n    print(f\"[{idx+1}/{len(schools_df)}] {school_name}\")\n    \n    # Initialize\n    city = state = latitude = longitude = enrollment = None\n    enrollment_source = location_source = None\n    \n    # --- ENROLLMENT: Priority 1 - Scrape ---\n    enrollment, scrape_result = scrape_enrollment_from_website(url)\n    if enrollment:\n        enrollment_source = scrape_result\n        print(f\"      ✓ Enrollment SCRAPED: {enrollment:,}\")\n    else:\n        print(f\"      Enrollment scrape: {scrape_result}\")\n    \n    # --- API for location and enrollment (if standalone school) ---\n    if is_standalone:\n        api_data, api_result = fetch_from_college_scorecard(school_name)\n        \n        if api_data:\n            # Location from API\n            city = api_data.get('city')\n            state = api_data.get('state')\n            latitude = api_data.get('latitude')\n            longitude = api_data.get('longitude')\n            if city and state:\n                location_source = f\"API ({api_data.get('api_name', '')})\"\n                print(f\"      ✓ Location from API: {city}, {state}\")\n            \n            # Enrollment from API (if not already scraped)\n            if not enrollment and api_data.get('enrollment'):\n                enrollment = api_data.get('enrollment')\n                enrollment_source = f\"API ({api_data.get('api_name', '')})\"\n                print(f\"      ✓ Enrollment from API: {enrollment:,}\")\n        else:\n            print(f\"      API: {api_result}\")\n    else:\n        print(f\"      API: Skipped (returns university-level data, not music school)\")\n    \n    # --- FALLBACK to reference file ---\n    if not city or not state:\n        city = ref_data.get('city')\n        state = ref_data.get('state')\n        latitude = ref_data.get('latitude')\n        longitude = ref_data.get('longitude')\n        location_source = 'reference file'\n        print(f\"      → Location FALLBACK: {city}, {state}\")\n    \n    if not enrollment:\n        enrollment = ref_data.get('enrollment')\n        enrollment_source = 'reference file'\n        print(f\"      → Enrollment FALLBACK: {enrollment:,}\")\n    \n    results.append({\n        'url': url,\n        'name': school_name,\n        'name_source': row['name_source'],\n        'city': city,\n        'state': state,\n        'latitude': latitude,\n        'longitude': longitude,\n        'location_source': location_source,\n        'enrollment': enrollment,\n        'enrollment_source': enrollment_source\n    })\n    \n    time.sleep(1)\n\n# Update schools_df with complete data\nschools_df = pd.DataFrame(results)\n\n# Summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\"*70)\n\nloc_api = schools_df['location_source'].str.contains('API', case=False, na=False).sum()\nloc_ref = schools_df['location_source'].str.contains('reference', case=False, na=False).sum()\nprint(f\"\\nLocation:   {loc_api}/10 from API, {loc_ref}/10 from reference file\")\n\nenr_scraped = schools_df['enrollment_source'].str.contains('scraped', case=False, na=False).sum()\nenr_api = schools_df['enrollment_source'].str.contains('API', case=False, na=False).sum()\nenr_ref = schools_df['enrollment_source'].str.contains('reference', case=False, na=False).sum()\nprint(f\"Enrollment: {enr_scraped}/10 scraped, {enr_api}/10 from API, {enr_ref}/10 from reference file\")\n\nprint(\"\\n\")\nschools_df[['name', 'city', 'state', 'enrollment', 'enrollment_source', 'location_source']]",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "---\n## Step 5: Fetch Weather Data",
      "id": "e43eb4a0-e5ea-4502-8954-a6febd93ad87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "GetWeatherData",
        "title": "GetWeatherData"
      },
      "outputs": [],
      "source": "# Collect the weather data, based on location, from the Open-Meteo API\n\ndef fetch_weather_data(latitude, longitude, school_name):\n    \"\"\"\n    Fetch weather from Open-Meteo API for January 2026\n    \"\"\"\n    url = 'https://archive-api.open-meteo.com/v1/archive'\n    \n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'start_date': '2026-01-01',\n        'end_date': '2026-01-31',\n        'daily': 'temperature_2m_max,temperature_2m_min,precipitation_sum',\n        'temperature_unit': 'fahrenheit',\n        'precipitation_unit': 'inch',\n        'timezone': 'America/New_York'\n    }\n    \n    try:\n        response = requests.get(url, params=params, timeout=10)\n        response.raise_for_status()\n        data = response.json()\n        \n        daily_data = data.get('daily', {})\n        \n        # List comprehension for efficiency\n        weather_records = [\n            {\n                'school_name': school_name,\n                'date': date,\n                'temp_max_f': temp_max,\n                'temp_min_f': temp_min,\n                'precipitation_inches': precip\n            }\n            for date, temp_max, temp_min, precip in zip(\n                daily_data.get('time', []),\n                daily_data.get('temperature_2m_max', []),\n                daily_data.get('temperature_2m_min', []),\n                daily_data.get('precipitation_sum', [])\n            )\n        ]\n        \n        print(f\"  {school_name}\")\n        return pd.DataFrame(weather_records)\n        \n    except Exception as e:\n        print(f\"  {school_name}: Error - {e}\")\n        return pd.DataFrame()\n\n\nprint(\"Fetching weather data...\\n\")\nall_weather_data = []\n\nfor idx, row in schools_df.iterrows():\n    weather_df = fetch_weather_data(\n        row['latitude'],\n        row['longitude'],\n        row['name']\n    )\n    \n    if not weather_df.empty:\n        all_weather_data.append(weather_df)\n    \n    time.sleep(0.5)\n\nweather_data_df = pd.concat(all_weather_data, ignore_index=True)\n\nprint(f\"\\nTotal weather records: {len(weather_data_df)}\")",
      "id": "83d8321b-9249-4ea1-8d4f-f0def2edf2d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "---\n## Step 6: Identify Severe Weather Days",
      "id": "3b65fefb-647c-4bb1-8ec1-301f28c8924e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "SevereWeatherDays",
        "title": "SevereWeatherDays"
      },
      "outputs": [],
      "source": "# Use vectorized operations, set severe weather from my documented standards above\nweather_data_df['extreme_cold'] = weather_data_df['temp_min_f'] < 20\nweather_data_df['full_freeze'] = weather_data_df['temp_max_f'] < 32\nweather_data_df['winter_precip'] = (\n    (weather_data_df['precipitation_inches'] > 0.5) & \n    (weather_data_df['temp_max_f'] < 32)\n)\n\nweather_data_df['is_severe_weather'] = (\n    weather_data_df['extreme_cold'] | \n    weather_data_df['full_freeze'] | \n    weather_data_df['winter_precip']\n)\n\nsevere_weather_df = weather_data_df[weather_data_df['is_severe_weather']].copy()\n\nprint(f\"Total severe weather days: {len(severe_weather_df)}\\n\")\nprint(\"By school:\")\nprint(severe_weather_df.groupby('school_name')['date'].count().sort_values(ascending=False))",
      "id": "a2027cba-c222-4f33-8f32-72ce61e4e1c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "---\n## Step 7: Aggregate Severe Weather",
      "id": "103b3376-716c-4e18-936b-81981974ee5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "AggregateSevereWeather",
        "title": "AggregateSevereWeather"
      },
      "outputs": [],
      "source": "#  Aggregate and count severe weather days\nsevere_weather_agg = (\n    severe_weather_df\n    .groupby('school_name')\n    .agg({\n        'date': lambda dates: ', '.join(sorted(dates)),\n        'is_severe_weather': 'count'\n    })\n    .rename(columns={\n        'date': 'severe_weather_dates',\n        'is_severe_weather': 'severe_weather_days_count'\n    })\n    .reset_index()\n)\n\nsevere_weather_agg",
      "id": "7af9941a-2196-47d7-ba62-71dd984fd0e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false
      },
      "source": "---\n## Step 8: Create Final Output",
      "id": "b13ea6e4-6d42-4419-8763-262b2e78c27b"
    },
    {
      "id": "9d04c425-bf31-42b1-ab04-06b5e6541515",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "StudentDaysImpacted",
        "title": "StudentDaysImpacted"
      },
      "source": "# schools_df already has enrollment, just rename and merge weather\nfinal_table = schools_df.rename(columns={'name': 'school_name'}).copy()\n\n# Merge with severe weather aggregation\nfinal_table = final_table.merge(severe_weather_agg, on='school_name', how='left')\n\n# Fill NaN for schools with no severe weather\nfinal_table['severe_weather_days_count'] = final_table['severe_weather_days_count'].fillna(0).astype(int)\nfinal_table['severe_weather_dates'] = final_table['severe_weather_dates'].fillna('None')\n\n# Calculate student-days impacted\nfinal_table['student_days_impacted'] = final_table['enrollment'] * final_table['severe_weather_days_count']\n\n# Sort by count of impacted days\nfinal_table = final_table.sort_values('student_days_impacted', ascending=False).reset_index(drop=True)\n\nprint(\"=\"*80)\nprint(\"FINAL RESULTS: Music Schools Winter Weather Impact (January 2026)\")\nprint(\"=\"*80)\nprint(f\"\\nSchools with severe weather: {(final_table['severe_weather_days_count'] > 0).sum()}/10\")\nprint(f\"Total student-days impacted: {final_table['student_days_impacted'].sum():,.0f}\\n\")\n\n# Display with formatted numbers\nfinal_table[['school_name', 'city', 'state', 'enrollment', 'severe_weather_days_count', 'student_days_impacted']].style.format({\n    'enrollment': '{:,.0f}',\n    'student_days_impacted': '{:,.0f}'\n})",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false
      },
      "source": "---\n## Step 9: Load to Snowflake",
      "id": "70eff201-a592-4083-a829-ca4767228cb9"
    },
    {
      "id": "dc45592a-e897-4a9e-955c-540e9f99870e",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "LoadForSQL",
        "title": "LoadForSQL"
      },
      "source": "# load to snowflake for SQL analysis\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nsession.sql(\"USE DATABASE SNOWBEARAIR_DB\").collect()\nsession.sql(\"USE SCHEMA PUBLIC\").collect()\n\n# Prepare data\nsnowflake_table = final_table.drop(columns=['enrollment_source'])\n\n# Create table\ntable_name = 'MUSIC_SCHOOLS_WINTER_WEATHER_IMPACT_JAN2026'\n\nsession.sql(f\"\"\"\nCREATE OR REPLACE TABLE {table_name} (\n    SCHOOL_NAME VARCHAR,\n    CITY VARCHAR,\n    STATE VARCHAR,\n    ENROLLMENT NUMBER,\n    SEVERE_WEATHER_DAYS_COUNT NUMBER,\n    SEVERE_WEATHER_DATES VARCHAR,\n    STUDENT_DAYS_IMPACTED NUMBER,\n    URL VARCHAR\n)\n\"\"\").collect()\n\n# Insert data\nfor idx, row in snowflake_table.iterrows():\n    session.sql(f\"\"\"\n    INSERT INTO {table_name} VALUES (\n        '{row['school_name'].replace(\"'\", \"''\")}',\n        '{row['city']}',\n        '{row['state']}',\n        {row['enrollment']},\n        {row['severe_weather_days_count']},\n        '{row['severe_weather_dates'].replace(\"'\", \"''\")}',\n        {row['student_days_impacted']},\n        '{row['url']}'\n    )\n    \"\"\").collect()\n\nprint(f\"Data loaded to: SNOWBEARAIR_DB.PUBLIC.{table_name}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "---\n## Step 10: SQL Analysis\n\n# First, present the Final Output as required by the Assignment Rubric\n# Then, conduct assorted other analyses so we know the impact to our trombone and tuba carrying bandmates!",
      "id": "02765a0b-7075-4bea-93e7-0bce36ade640"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "FinalOutput",
        "title": "FinalOutput"
      },
      "outputs": [],
      "source": "%%sql\n\n# For each school, where is it, total music school enrollment, number of days of severe weather, and total student days impacted\nSELECT \n    SCHOOL_NAME,\n    CITY,\n    STATE,\n    TO_VARCHAR(ENROLLMENT, '999,999') AS ENROLLMENT,\n    SEVERE_WEATHER_DAYS_COUNT,\n    TO_VARCHAR(STUDENT_DAYS_IMPACTED, '999,999') AS STUDENT_DAYS_IMPACTED\nFROM MUSIC_SCHOOLS_WINTER_WEATHER_IMPACT_JAN2026\nORDER BY STUDENT_DAYS_IMPACTED DESC;",
      "id": "e91c0d9e-a025-4b87-a2ea-d9c0bbfbc75e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "DaysByState",
        "title": "DaysByState"
      },
      "outputs": [],
      "source": "%%sql\n#  Same data, but by state - which combines some schools together\nWITH state_summary AS (\n    SELECT \n        STATE,\n        COUNT(*) AS school_count,\n        SUM(ENROLLMENT) AS total_enrollment,\n        SUM(STUDENT_DAYS_IMPACTED) AS total_impact\n    FROM MUSIC_SCHOOLS_WINTER_WEATHER_IMPACT_JAN2026\n    GROUP BY STATE\n)\nSELECT \n    STATE,\n    school_count,\n    TO_VARCHAR(total_enrollment, '999,999') AS total_enrollment,\n    TO_VARCHAR(total_impact, '999,999') AS total_impact,\n    ROUND(total_impact / NULLIF(total_enrollment, 0), 1) AS avg_days_per_student\nFROM state_summary\nORDER BY total_impact DESC;",
      "id": "95865bae-c528-4c3c-b136-564387f307ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "PercentOfTotal",
        "title": "PercentOfTotal"
      },
      "outputs": [],
      "source": "%%sql\n#  Percentage of total impacted days, but only for those schools with any impact at all\nWITH impact_metrics AS (\n    SELECT \n        SCHOOL_NAME,\n        ENROLLMENT,\n        SEVERE_WEATHER_DAYS_COUNT,\n        STUDENT_DAYS_IMPACTED\n    FROM MUSIC_SCHOOLS_WINTER_WEATHER_IMPACT_JAN2026\n    WHERE SEVERE_WEATHER_DAYS_COUNT > 0\n)\nSELECT \n    SCHOOL_NAME,\n    TO_VARCHAR(ENROLLMENT, '999,999') AS ENROLLMENT,\n    SEVERE_WEATHER_DAYS_COUNT,\n    TO_VARCHAR(STUDENT_DAYS_IMPACTED, '999,999') AS STUDENT_DAYS_IMPACTED,\n    ROUND(100.0 * STUDENT_DAYS_IMPACTED / SUM(STUDENT_DAYS_IMPACTED) OVER (), 1) AS pct_of_total\nFROM impact_metrics\nORDER BY STUDENT_DAYS_IMPACTED DESC;",
      "id": "7db3c475-3b16-428b-b1ae-7d570c71adcc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "---\n## Summary\n\n### Data Collection Results\n\n| Data | Primary Source | Fallback | Actual Result |\n|------|---------------|----------|---------------|\n## | School Names | Web scraping | Reference CSV | ~8/10 scraped |\n## | Enrollment | Web scraping → API | Reference CSV | 7/10 scraped, 2/10 API, 1/10 fallback |\n## | Coordinates | Reference CSV | N/A | 10/10 |\n## | Weather | Open-Meteo API | N/A | 10/10 |\n\n**Note:** Due to Snowflake network restrictions, scraping and API calls were blocked in the cloud environment. \nThe pipeline was run locally in a Jupyter Notebook to demonstrate full functionality. \nThe locally-generated results (`music_schools_weather_results.csv`) were then imported into Snowflake for SQL analysis.\n\n### Enrollment Data Sources (from local run)\n\n| School | Source | Enrollment |\n|--------|--------|------------|\n| Curtis Institute of Music | Scraped | 160 |\n| Oberlin Conservatory of Music | Scraped | 540 |\n| New England Conservatory of Music | Scraped | 850 |\n| Manhattan School of Music | Scraped | 960 |\n| Indiana University Jacobs School of Music | Scraped | 1,500 |\n| Eastman School of Music | Scraped | 260 |\n| San Francisco Conservatory of Music | Scraped | 460 |\n| The Juilliard School | API | 460 |\n| Berklee College of Music | API | 7,510 |\n| USC Thornton School of Music | Fallback | 1,069 |\n\n### Data Engineering Challenges\n\n1. **Network Restrictions:** Snowflake cloud environment inconsistently blocked external website access\n2. **API Rate Limits:** College Scorecard API appears to limit requests by some measure or means (I'm not sure)\n3. **Inconsistent HTML:** Each site requires custom scraping patterns\n4. **Data in PDFs:** Some schools publish enrollment in PDF format only\n5. **University vs School Data:** APIs return full university enrollment, not music school specifically (e.g., USC)\n\n### Solution: Hybrid Approach\n\nTo overcome Snowflake's network restrictions while demonstrating full pipeline functionality:\n1. I ran the complete pipeline locally in a Jupyter Notebook\n2. It ran beautifully!\n3. I exported results to CSV (`music_schools_weather_results.csv`)\n4. Imported CSV into Snowflake for SQL analysis and visualization\n\n### Key Insights\n\n1. A Google search shows enrollment data instantly, but programmatic access requires navigating rate limits, firewalls, and inconsistent data formats. \n2. Now I understand why data engineers say \"80% of the work is getting the data.\"\n3. If you are going to play tuba, trombone or cello, you should go to music school in California!\n\n### Next Up\n\nI selected some visualizations to enhance the presentation of this data analysis\n",
      "id": "b66bb084-a61a-4686-890d-4a9c9a3ebde6"
    },
    {
      "id": "64d0702e-407a-4a13-97a3-3848ff1ab1e0",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "VizStudentDays",
        "title": "VizStudentDays"
      },
      "source": "# =============================================================================\n# VISUALIZATION 1: Horizontal Bar Chart - Student-Days Impacted\n# =============================================================================\n# This is the \"so what\" chart - shows total impact by school\n\n# Sort by impact for visual hierarchy\nviz_data = final_table.sort_values('student_days_impacted', ascending=True)\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Create horizontal bars\nbars = ax.barh(\n    viz_data['school_name'], \n    viz_data['student_days_impacted'],\n    color='#2E86AB'\n)\n\n# Highlight California schools (zero impact) in different color\nfor i, (school, impact) in enumerate(zip(viz_data['school_name'], viz_data['student_days_impacted'])):\n    if impact == 0:\n        bars[i].set_color('#E8E8E8')\n\n# Add value labels on bars\nfor i, (school, impact) in enumerate(zip(viz_data['school_name'], viz_data['student_days_impacted'])):\n    if impact > 0:\n        ax.text(impact + 1500, i, f'{impact:,.0f}', va='center', fontsize=9)\n    else:\n        ax.text(impact + 1500, i, 'No severe weather', va='center', fontsize=9, color='gray')\n\n# Formatting\nax.set_xlabel('Student-Days Impacted', fontsize=12)\nax.set_title('Winter Weather Impact on Top Music Schools\\nJanuary 2026', fontsize=14, fontweight='bold')\nax.set_xlim(0, max(viz_data['student_days_impacted']) * 1.15)\n\n# Remove top and right spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f87c83b-210d-4dc6-9daa-9893d5eb7ba1",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "VizSevereWeatherDays",
        "title": "VizSevereWeatherDays"
      },
      "source": "# =============================================================================\n# VISUALIZATION 2: Horizontal Bar Chart - Severe Weather Days\n# =============================================================================\n# This removes enrollment bias - shows raw weather severity\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Sort by severe days\nviz_data = final_table.sort_values('severe_weather_days_count', ascending=True)\n\n# Create horizontal bars with gradient based on severity\ncolors = ['#E8E8E8' if d == 0 else plt.cm.Blues(0.3 + (d / 30) * 0.7) \n          for d in viz_data['severe_weather_days_count']]\n\nbars = ax.barh(\n    viz_data['school_name'], \n    viz_data['severe_weather_days_count'],\n    color=colors\n)\n\n# Add value labels\nfor i, days in enumerate(viz_data['severe_weather_days_count']):\n    if days > 0:\n        ax.text(days + 0.3, i, f'{days} days', va='center', fontsize=9)\n    else:\n        ax.text(days + 0.3, i, 'None', va='center', fontsize=9, color='gray')\n\n# Formatting\nax.set_xlabel('Severe Weather Days in January 2026', fontsize=12)\nax.set_title('Severe Winter Weather Days by Music School Location\\nJanuary 2026', fontsize=14, fontweight='bold')\nax.set_xlim(0, 31)\n\n# Add reference line for half the month\nax.axvline(x=15.5, color='red', linestyle='--', alpha=0.5, label='Half of January')\nax.legend(loc='lower right')\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a85d4c23-7d13-4bcb-9d21-0d590bf0eac7",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "VizEnrollmentSevereDaysScatterPlot",
        "title": "VizEnrollmentSevereDaysScatterPlot"
      },
      "source": "# =============================================================================\n# VISUALIZATION 3: Scatter Plot - Enrollment vs Severe Days\n# =============================================================================\n# Shows relationship between school size and weather exposure\n# My favorite, by far!\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Filter out zero-impact schools for cleaner viz (or keep them - your choice)\n# viz_data = final_table[final_table['severe_weather_days_count'] > 0]\nviz_data = final_table.copy()\n\n# Bubble size based on student-days impacted (scaled for visibility)\nsizes = (viz_data['student_days_impacted'] / 500) + 50\n\n# Color based on severe weather days\ncolors = viz_data['severe_weather_days_count']\n\nscatter = ax.scatter(\n    viz_data['enrollment'],\n    viz_data['severe_weather_days_count'],\n    s=sizes,\n    c=colors,\n    cmap='YlOrRd',\n    alpha=0.7,\n    edgecolors='black',\n    linewidth=0.5\n)\n\n# Add school labels\nfor idx, row in viz_data.iterrows():\n    # Shorten long names\n    name = row['school_name'].replace('School of Music', '').replace('College of Music', '')\n    name = name.replace('Conservatory of Music', 'Conserv.').replace('Institute of Music', 'Inst.')\n    name = name.strip()\n    \n    # Offset labels to avoid overlap\n    offset_x = 100\n    offset_y = 0.5\n    \n    ax.annotate(\n        name,\n        (row['enrollment'], row['severe_weather_days_count']),\n        xytext=(offset_x, offset_y),\n        textcoords='offset points',\n        fontsize=8,\n        alpha=0.8\n    )\n\n# Add colorbar\ncbar = plt.colorbar(scatter, ax=ax)\ncbar.set_label('Severe Weather Days', fontsize=10)\n\n# Formatting\nax.set_xlabel('Total Enrollment', fontsize=12)\nax.set_ylabel('Severe Weather Days', fontsize=12)\nax.set_title('Enrollment vs Weather Severity\\nBubble Size = Student-Days Impacted', fontsize=14, fontweight='bold')\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add grid for readability\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0876736-4922-4f8c-9f30-6e1bd9f92654",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "JanuaryYoYComparison",
        "title": "JanuaryYoYComparison"
      },
      "source": "# =============================================================================\n# STRETCH IDEA 1: Compare January 2026 to January 2025 (\"Normal\" Year)\n# Not much is different\n# =============================================================================\n\ndef fetch_weather_for_year(latitude, longitude, school_name, year):\n    \"\"\"\n    Fetch weather data for a specific January\n    \"\"\"\n    url = 'https://archive-api.open-meteo.com/v1/archive'\n    \n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'start_date': f'{year}-01-01',\n        'end_date': f'{year}-01-31',\n        'daily': 'temperature_2m_max,temperature_2m_min,precipitation_sum',\n        'temperature_unit': 'fahrenheit',\n        'precipitation_unit': 'inch',\n        'timezone': 'America/New_York'\n    }\n    \n    try:\n        response = requests.get(url, params=params, timeout=10)\n        response.raise_for_status()\n        data = response.json()\n        \n        daily_data = data.get('daily', {})\n        \n        weather_records = [\n            {\n                'school_name': school_name,\n                'year': year,\n                'date': date,\n                'temp_max_f': temp_max,\n                'temp_min_f': temp_min,\n                'precipitation_inches': precip\n            }\n            for date, temp_max, temp_min, precip in zip(\n                daily_data.get('time', []),\n                daily_data.get('temperature_2m_max', []),\n                daily_data.get('temperature_2m_min', []),\n                daily_data.get('precipitation_sum', [])\n            )\n        ]\n        \n        return pd.DataFrame(weather_records)\n        \n    except Exception as e:\n        print(f\"  Error for {school_name} ({year}): {e}\")\n        return pd.DataFrame()\n\n\n# Fetch January 2025 data for comparison\nprint(\"Fetching January 2025 weather data for comparison...\\n\")\nweather_2025_list = []\n\nfor idx, row in schools_df.iterrows():\n    print(f\"  {row['name']}...\")\n    weather_df = fetch_weather_for_year(\n        row['latitude'],\n        row['longitude'],\n        row['name'],\n        2025\n    )\n    if not weather_df.empty:\n        weather_2025_list.append(weather_df)\n    time.sleep(0.5)\n\nweather_2025_df = pd.concat(weather_2025_list, ignore_index=True)\n\n# Apply same severe weather criteria to 2025\nweather_2025_df['extreme_cold'] = weather_2025_df['temp_min_f'] < 20\nweather_2025_df['full_freeze'] = weather_2025_df['temp_max_f'] < 32\nweather_2025_df['winter_precip'] = (\n    (weather_2025_df['precipitation_inches'] > 0.5) & \n    (weather_2025_df['temp_max_f'] < 32)\n)\nweather_2025_df['is_severe_weather'] = (\n    weather_2025_df['extreme_cold'] | \n    weather_2025_df['full_freeze'] | \n    weather_2025_df['winter_precip']\n)\n\n# Aggregate 2025 severe days\nsevere_2025_agg = (\n    weather_2025_df[weather_2025_df['is_severe_weather']]\n    .groupby('school_name')\n    .agg(severe_days_2025=('is_severe_weather', 'count'))\n    .reset_index()\n)\n\n# Merge with final table to create comparison\ncomparison_df = final_table[['school_name', 'city', 'state', 'severe_weather_days_count']].copy()\ncomparison_df = comparison_df.rename(columns={'severe_weather_days_count': 'severe_days_2026'})\ncomparison_df = comparison_df.merge(severe_2025_agg, on='school_name', how='left')\ncomparison_df['severe_days_2025'] = comparison_df['severe_days_2025'].fillna(0).astype(int)\n\n# Calculate difference\ncomparison_df['difference'] = comparison_df['severe_days_2026'] - comparison_df['severe_days_2025']\ncomparison_df['pct_change'] = (\n    (comparison_df['difference'] / comparison_df['severe_days_2025'].replace(0, 1)) * 100\n).round(1)\n\n# Add interpretation\ncomparison_df['assessment'] = comparison_df['difference'].apply(\n    lambda x: 'Much Worse' if x >= 5 else ('Worse' if x > 0 else ('Same' if x == 0 else 'Better'))\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"YEAR-OVER-YEAR COMPARISON: January 2026 vs January 2025\")\nprint(\"=\"*80 + \"\\n\")\n\ncomparison_df = comparison_df.sort_values('difference', ascending=False)\nprint(comparison_df[['school_name', 'state', 'severe_days_2025', 'severe_days_2026', 'difference', 'assessment']].to_string(index=False))\n\n# Summary statistics\navg_2025 = comparison_df['severe_days_2025'].mean()\navg_2026 = comparison_df['severe_days_2026'].mean()\nprint(f\"\\nAverage severe days across all schools:\")\nprint(f\"  January 2025: {avg_2025:.1f} days\")\nprint(f\"  January 2026: {avg_2026:.1f} days\")\nprint(f\"  Difference:   {avg_2026 - avg_2025:+.1f} days\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "65c3a596-29ac-475a-8d0d-b5fc441bba4f",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "SeverityIndex",
        "title": "SeverityIndex"
      },
      "source": "# =============================================================================\n# STRETCH IDEA 2: Severity Index\n# =============================================================================\n# Creates a weighted score based on HOW extreme the weather was\n# Factors:\n#   1. Extreme cold intensity (how far below 20°F)\n#   2. Consecutive severe days (longer streaks = worse)\n#   3. Total precipitation during freezing conditions\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SEVERITY INDEX CALCULATION\")\nprint(\"=\"*80 + \"\\n\")\n\ndef calculate_severity_index(school_weather_df):\n    \"\"\"\n    Calculate a severity index (0-100) based on:\n    - Cold intensity: How far below thresholds\n    - Duration: Consecutive severe days\n    - Precipitation: Snow/ice accumulation\n    \"\"\"\n    if school_weather_df.empty:\n        return 0\n    \n    # Component 1: Cold Intensity Score (0-40 points)\n    # Based on how far minimum temps dropped below 20°F\n    min_temp = school_weather_df['temp_min_f'].min()\n    if min_temp < 20:\n        cold_intensity = min(40, (20 - min_temp) * 2)  # 2 points per degree below 20\n    else:\n        cold_intensity = 0\n    \n    # Component 2: Duration Score (0-30 points)\n    # Based on longest streak of consecutive severe days\n    severe_days = school_weather_df['is_severe_weather'].values\n    max_streak = 0\n    current_streak = 0\n    for day in severe_days:\n        if day:\n            current_streak += 1\n            max_streak = max(max_streak, current_streak)\n        else:\n            current_streak = 0\n    duration_score = min(30, max_streak * 3)  # 3 points per consecutive day, max 30\n    \n    # Component 3: Precipitation Score (0-30 points)\n    # Based on total precipitation during freezing conditions\n    freezing_precip = school_weather_df[\n        school_weather_df['temp_max_f'] < 32\n    ]['precipitation_inches'].sum()\n    precip_score = min(30, freezing_precip * 10)  # 10 points per inch, max 30\n    \n    total_score = cold_intensity + duration_score + precip_score\n    \n    return {\n        'cold_intensity': round(cold_intensity, 1),\n        'duration_score': round(duration_score, 1),\n        'precip_score': round(precip_score, 1),\n        'severity_index': round(total_score, 1)\n    }\n\n\n# Calculate severity index for each school\nseverity_data = []\n\nfor school_name in final_table['school_name']:\n    school_weather = weather_data_df[weather_data_df['school_name'] == school_name].copy()\n    scores = calculate_severity_index(school_weather)\n    scores['school_name'] = school_name\n    severity_data.append(scores)\n\nseverity_df = pd.DataFrame(severity_data)\n\n# Merge with final table\nseverity_results = final_table[['school_name', 'city', 'state', 'enrollment', 'severe_weather_days_count']].merge(\n    severity_df, on='school_name'\n)\n\n# Sort by severity index\nseverity_results = severity_results.sort_values('severity_index', ascending=False)\n\n# Add severity category\ndef categorize_severity(score):\n    if score >= 60:\n        return 'Extreme'\n    elif score >= 40:\n        return 'Severe'\n    elif score >= 20:\n        return 'Moderate'\n    elif score > 0:\n        return 'Mild'\n    else:\n        return 'None'\n\nseverity_results['severity_category'] = severity_results['severity_index'].apply(categorize_severity)\n\nprint(\"Severity Index Components:\")\nprint(\"  - Cold Intensity (0-40): How far temps dropped below 20°F\")\nprint(\"  - Duration Score (0-30): Longest streak of consecutive severe days\")\nprint(\"  - Precipitation Score (0-30): Total precipitation during freezing temps\")\nprint(\"  - Total Severity Index: 0-100 scale\\n\")\n\nprint(severity_results[[\n    'school_name', 'state', 'cold_intensity', 'duration_score', \n    'precip_score', 'severity_index', 'severity_category'\n]].to_string(index=False))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "03cd58c5-d7d8-4fa8-8821-bca6bd5e5cdc",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "VizYoY",
        "title": "VizYoY"
      },
      "source": "# =============================================================================\n# VISUALIZATION: Year-over-Year Comparison\n# =============================================================================\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Prepare data\ncomp_viz = comparison_df.sort_values('severe_days_2026', ascending=True)\ny_pos = range(len(comp_viz))\nbar_height = 0.35\n\n# Create grouped bars\nbars_2025 = ax.barh([p - bar_height/2 for p in y_pos], comp_viz['severe_days_2025'], \n                     bar_height, label='January 2025', color='#7FB3D5', alpha=0.8)\nbars_2026 = ax.barh([p + bar_height/2 for p in y_pos], comp_viz['severe_days_2026'], \n                     bar_height, label='January 2026', color='#2E86AB', alpha=0.8)\n\n# Add value labels\nfor i, (d25, d26) in enumerate(zip(comp_viz['severe_days_2025'], comp_viz['severe_days_2026'])):\n    ax.text(d25 + 0.3, i - bar_height/2, str(int(d25)), va='center', fontsize=9)\n    ax.text(d26 + 0.3, i + bar_height/2, str(int(d26)), va='center', fontsize=9)\n\nax.set_yticks(y_pos)\nax.set_yticklabels(comp_viz['school_name'])\nax.set_xlabel('Severe Weather Days', fontsize=12)\nax.set_title('January 2026 vs January 2025: Severe Weather Days\\nby Music School Location', fontsize=14, fontweight='bold')\nax.legend(loc='lower right')\nax.set_xlim(0, 35)\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "634678e7-78ac-4efe-98a3-97feba0f2105",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "VizSeverityIndex",
        "title": "VizSeverityIndex"
      },
      "source": "# =============================================================================\n# VISUALIZATION: Severity Index\n# =============================================================================\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Sort and prepare data\nsev_viz = severity_results.sort_values('severity_index', ascending=True)\n\n# Color by category\ncategory_colors = {\n    'Extreme': '#C0392B',\n    'Severe': '#E74C3C',\n    'Moderate': '#F39C12',\n    'Mild': '#F1C40F',\n    'None': '#E8E8E8'\n}\ncolors = [category_colors[cat] for cat in sev_viz['severity_category']]\n\n# Create stacked horizontal bars\nbars = ax.barh(sev_viz['school_name'], sev_viz['severity_index'], color=colors, edgecolor='black', linewidth=0.5)\n\n# Add value labels\nfor i, (score, cat) in enumerate(zip(sev_viz['severity_index'], sev_viz['severity_category'])):\n    label = f'{score:.0f} ({cat})'\n    ax.text(score + 1, i, label, va='center', fontsize=9)\n\nax.set_xlabel('Severity Index (0-100)', fontsize=12)\nax.set_title('Winter Weather Severity Index by Music School\\nJanuary 2026', fontsize=14, fontweight='bold')\nax.set_xlim(0, 100)\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=color, label=cat) for cat, color in category_colors.items() if cat != 'None']\nax.legend(handles=legend_elements, loc='lower right', title='Severity')\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}